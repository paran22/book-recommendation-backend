{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06FarYvdmTQq",
        "outputId": "7d9a4d9a-0009-44b1-b0db-297573b8031a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.17.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.28.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.6.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.19.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install wandb datasets transformers trl torch peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BlTmKFl5mTQr"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "import torch\n",
        "from peft import LoraConfig, get_peft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OsCXN4RynNgh"
      },
      "outputs": [],
      "source": [
        "# !huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8p9PWwYmTQr",
        "outputId": "18a1dda6-1702-4e9c-f45f-1ffa7eeae744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# 모델과 토크나이저 로드\n",
        "model_name = \"gpt2-medium\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Cia6RQwmTQs",
        "outputId": "5525558e-4828-48e9-8133-10430e96e384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 데이터 크기: 125\n",
            "학습 데이터 크기: 100\n",
            "테스트 데이터 크기: 25\n",
            "\n",
            "=== 학습 데이터 예시 ===\n",
            "{'input': '감정: 혼란스러움 / 원하는 효과: 위로받고 싶다 / 직업: 구직자 / 현재 상황: 스트레스 해소 / 집중도: 빠르게 몰입할 수 있는 책', 'instruction': '혼란스러운 심리 상태에서 위안을 얻고 싶을 때 적절한 3권을 골라, 추천 이유와 효과를 적으세요.', 'output': '1. 마음은 괜찮냐고 시가 물었다 - 황인환\\n\\n* 추천 이유: 시를 통해 감정의 혼란을 천천히 정돈할 수 있습니다.\\n* 이 책이 도움이 될 수 있는 이유: 짧은 글이 매일의 위로가 되어줄 수 있습니다.\\n\\n2. 오렌지와 빵칼 - 청예\\n\\n* 추천 이유: 다양한 인물의 시선을 따라가며 자신과 상황을 분리해 생각할 수 있습니다.\\n* 이 책이 도움이 될 수 있는 이유: 쉽게 몰입할 수 있는 이야기로 감정적 환기를 제공합니다.\\n\\n3. 이렇게 누워만 있어도 괜찮을까 - 안예슬\\n\\n* 추천 이유: 혼란스러운 변화 속에서 자신에게 더 관대해지는 법을 일깨워 줍니다.\\n* 이 책이 도움이 될 수 있는 이유: 편안한 휴식의 순간을 통해 스스로 회복력을 느낄 수 있습니다.'}\n",
            "\n",
            "=== 테스트 데이터 예시 ===\n",
            "{'instruction': '슬럼프에 빠진 운동선수나 직장인에게 자기 극복과 성장 동기를 줄 책을 추천해줘.', 'input': '감정: 슬럼프, 무기력\\n직업: 운동선수, 직장인\\n효과: 자기 극복, 동기부여\\n상황: 경기 또는 업무 후\\n집중도: 몰입감 높은 책', 'output': '1. 운동의 진실과 기쁨 - 강윤규 지음\\n   - 추천 이유: 운동을 통한 자기성찰과 회복의 과정이 담긴 책입니다.\\n   - 이 책이 도움이 될 수 있는 이유: 운동의 본질적 기쁨과 극복 경험을 통해 재충전할 수 있습니다.\\n2. 해나가는 힘 : 늘 사랑받고 싶은 사람을 위한 심리학 - 이승현\\n   - 추천 이유: 자신을 사랑하고 자신감과 추진력을 키울 수 있도록 돕습니다.\\n   - 이 책이 도움이 될 수 있는 이유: 슬럼프 극복을 위한 심리적 기초 체력을 키워줍니다.\\n3. 바람이 강하게 불고 있다 - 미우라 시온\\n   - 추천 이유: 젊은이들의 도전과 팀워크를 그린 소설로, 다시 일어설 용기를 줍니다.\\n   - 이 책이 도움이 될 수 있는 이유: 좌절에서 벗어나 상승의 기회를 포착할 수 있습니다.'}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "\n",
        "# corpus.json 파일 읽기\n",
        "with open('./data/corpus.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# train/test 분리 (80:20 비율)\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"전체 데이터 크기: {len(data)}\")\n",
        "print(f\"학습 데이터 크기: {len(train_data)}\")\n",
        "print(f\"테스트 데이터 크기: {len(test_data)}\")\n",
        "\n",
        "# 데이터 예시 출력\n",
        "print(\"\\n=== 학습 데이터 예시 ===\")\n",
        "print(train_data[0])\n",
        "print(\"\\n=== 테스트 데이터 예시 ===\")\n",
        "print(test_data[0])\n",
        "\n",
        "# 리스트를 Dataset 형식으로 변환\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "test_dataset = Dataset.from_list(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iGOEjDggmTQs"
      },
      "outputs": [],
      "source": [
        "# 프롬프트 포매팅 함수 정의\n",
        "def formatting_func(example):\n",
        "    # instruction, input, output이 단일 문자열인 경우를 처리\n",
        "    instruction = example['instruction']\n",
        "    input_text = example['input'].strip() if example['input'] else \"\"\n",
        "    output = example['output']\n",
        "\n",
        "    # 형식화된 프롬프트 생성\n",
        "    text = f\"[Instruction]\\n{instruction.strip()}\\n\\n\"\n",
        "    if input_text:\n",
        "        text += f\"[Input]\\n{input_text}\\n\\n\"\n",
        "    text += f\"[Output]\\n{output.strip()}\"\n",
        "\n",
        "    return text\n",
        "\n",
        "# 데이터 콜레이터 설정\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # Causal Language Modeling을 위해 False로 설정\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "om8dOCbEmTQs"
      },
      "outputs": [],
      "source": [
        "# import gc\n",
        "\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# wandb.init(\n",
        "#     project='Hanghae99-book-recommandation',\n",
        "#     name=f'gpt-finetuning',\n",
        "#     reinit=True\n",
        "# )\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_name,\n",
        "# )\n",
        "\n",
        "# trainer = SFTTrainer(\n",
        "#     model,\n",
        "#     train_dataset=train_dataset,\n",
        "#     eval_dataset=test_dataset,\n",
        "#     args=SFTConfig(\n",
        "#         output_dir=f\"/tmp/clm-instruction-tuning\",\n",
        "#         max_seq_length=128,\n",
        "#         eval_strategy=\"epoch\",\n",
        "#         save_strategy=\"epoch\",\n",
        "#         logging_steps=10,\n",
        "#         num_train_epochs=3,\n",
        "#         learning_rate=2e-5,\n",
        "#         per_device_train_batch_size=1,\n",
        "#         gradient_accumulation_steps=4,\n",
        "#         load_best_model_at_end=True,\n",
        "#         metric_for_best_model=\"eval_loss\"\n",
        "#     ),\n",
        "#     formatting_func=formatting_func,\n",
        "#     data_collator=data_collator,\n",
        "# )\n",
        "\n",
        "# # 학습 시작\n",
        "# train_result = trainer.train()\n",
        "# metrics = train_result.metrics\n",
        "# trainer.log_metrics(\"train\", metrics)\n",
        "\n",
        "# trainer.save_model()\n",
        "\n",
        "# wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jlpPlVJ5DN9"
      },
      "source": [
        "### LoRA 적용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BmY9vsnO5DN9"
      },
      "outputs": [],
      "source": [
        "def train_with_lora(lora_r, model_name, train_dataset, test_dataset, formatting_func, data_collator):\n",
        "    # 디바이스 자동 선택: CUDA > MPS > CPU\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        torch_dtype = torch.float16  # CUDA에서는 float16이 빠르고 효율적\n",
        "        use_fp16 = True\n",
        "        device_map = 'auto'\n",
        "        print(\"CUDA를 사용합니다.\")\n",
        "    elif getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        torch_dtype = torch.float32  # MPS에서는 float32가 안전함\n",
        "        use_fp16 = False\n",
        "        device_map = None\n",
        "        print(\"MPS를 사용합니다.\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        torch_dtype = torch.float32\n",
        "        use_fp16 = False\n",
        "        device_map = None\n",
        "        print(\"CPU를 사용합니다.\")\n",
        "\n",
        "    wandb.init(\n",
        "        project='Hanghae99-book-recommandation',\n",
        "        name=f'{model_name}-with-lora-r{lora_r}',\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    print(f\"\\n=== Training with LoRA rank {lora_r} ===\")\n",
        "\n",
        "    # 모델 로드 (device_map이 있을 때만 넣음)\n",
        "    model_kwargs = {\n",
        "        \"torch_dtype\": torch_dtype\n",
        "    }\n",
        "    if device_map is not None:\n",
        "        model_kwargs[\"device_map\"] = device_map\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        **model_kwargs\n",
        "    )\n",
        "\n",
        "    # device_map이 없으면 명시적으로 모델 이동\n",
        "    if device_map is None:\n",
        "        model = model.to(device)\n",
        "\n",
        "    # LoRA 설정\n",
        "    lora_config = LoraConfig(\n",
        "        r=lora_r,\n",
        "        lora_alpha=32,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    # SFTTrainer 정의\n",
        "    trainer = SFTTrainer(\n",
        "        model,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        args=SFTConfig(\n",
        "            output_dir=f\"/tmp/clm-instruction-tuning-lora-{lora_r}\",\n",
        "            max_seq_length=128,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_steps=10,\n",
        "            num_train_epochs=3,\n",
        "            learning_rate=5e-5,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            per_device_train_batch_size=1,  # MPS/CPU 환경은 1, CUDA는 4까지도 가능(필요시 조건 분기)\n",
        "            gradient_accumulation_steps=4,\n",
        "            fp16=use_fp16\n",
        "        ),\n",
        "        formatting_func=formatting_func,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    # 모델이 올바른 device로 있는지 추가 확인 필요 시 trainer.model = trainer.model.to(device)\n",
        "    if device_map is None:\n",
        "        trainer.model = trainer.model.to_empty(device)\n",
        "\n",
        "    # 학습 시작\n",
        "    train_result = trainer.train()\n",
        "    metrics = train_result.metrics\n",
        "    trainer.log_metrics(\"train\", metrics)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        print('Max Alloc:', round(torch.cuda.max_memory_allocated(0)/1024**3, 1), 'GB')\n",
        "\n",
        "    # 모델 저장\n",
        "    trainer.save_model()\n",
        "\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jvpOLMDyqlp",
        "outputId": "7818b4dc-d33f-4ce7-d718-bc4499007b18"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) \n",
            "Token is valid (permission: read).\n",
            "The token `mac` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `mac`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jD1-r8qt5DN9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "2d2bca36-4ed8-45c6-be4e-8ab004aa364f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_with_lora' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f2068c3f7edf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_with_lora(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mlora_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"google/gemma-7b-it\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_with_lora' is not defined"
          ]
        }
      ],
      "source": [
        "train_with_lora(\n",
        "    lora_r=8,\n",
        "    model_name=\"google/gemma-7b-it\",\n",
        "    train_dataset=train_dataset,\n",
        "    test_dataset=test_dataset,\n",
        "    formatting_func=formatting_func,\n",
        "    data_collator=data_collator\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}